#+title: Quantification and Evaluation of Large Language Model Hallucinations via Targeted Domain Knowledge Prompting

* Introduction
** General Premise
This capstone project seeks to explore methodologies pertaining to the quantification and evaluation of hallucination rates within Large Language Models, with the ultimate end-goal being a dataset which produces reliable metrics for the hallucination rate of these models and can be either improved or extended into a proper benchmark dataset.

** Motivation
The centering of Machine-Learning in the cultural zeitgeist belies the complexity of the underlying model architectures - for end-users, they are often available as an API, or, at "best/worst", a browser interface. This, combined with the tendency on the part of larger companies (i.e., the ones producing LLMs) to obfuscate their architectures and methodologies (and to play up & mystify the abilities of their products), means that there is a certain - not quite ignorance, more like confusion - surrounding the particular function and scope of these models. *As such, there is a tendency to see LLMs like GPT used as rudimentary QA systems: to report facts[source], assist in prose [source], to even write code [source].*

We see this practice as fruitful for the study of hallucinations - often, these questions are (a.) domain-specific, and (b.) prompted to untuned, general-form
versions of these models. As such, it may be possible to develop a dataset of prompts to these pseudo-QA systems.

** Prototype Methodology
1. Develop schematizations for "Domain Knowledge" and "Hallucinations" which can accommodate each other and capture all significant phenomenon in the latter.
2. Develop an experimental methodology (capable of being run) to gather hallucination-generating questions from annotators
   NOTE - strict adherence to this part of the project might be a bit too committed to the theatre of "the experiment." Explore other options (just write a ton of prompts, feed the questions directly to the LLM, etc).
3. Build a pipeline that can evaluate several LLMs (without additional training) on this dataset. Develop an evaluation metric that captures nuances expressed through the hallucination schema.
4. Interpret the results in relation to the domain schema and try to frame the next steps of the project.

* Current Notes
** What is Domain Knowledge?
** What are Hallucinations?


* Roadmap
- Lit Review => 02/26
