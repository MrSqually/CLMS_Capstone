#+title: Lit Review
#+author: Dean Cahill

* Sources gathered via:
@misc{MinerviniAHD2014,
  author = {Pasquale Minervini and others},
  title = {awesome-hallucination-detection},
  year = {2014},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/EdinburghNLP/awesome-hallucination-detection}}
}



* HaluEval: A Large-Scale Hallucination Evaluation Benchmark
** Citation
@misc{li2023halueval,
      title={HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models},
      author={Junyi Li and Xiaoxue Cheng and Wayne Xin Zhao and Jian-Yun Nie and Ji-Rong Wen},
      year={2023},
      eprint={2305.11747},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

** Summary
- Benchmark of 35,000 hallucinated/normal samples
  - 5000 general user queries
  - 30000 task specific examples
    - QA
    - Knowledge Grounded Dialogue
    - Summarization
- LLMs poor at distinguishing hallucinations
  - 58% : barely above chance
  - Spans that look similar but differ in key facts

[[.images/halueval.png]]
[[.images/halueval2.png]]
* Natural Questions: A Benchmark for QA Research
** Citation`
@article{kwiatkowski-etal-2019-natural,
    title = "Natural Questions: A Benchmark for Question Answering Research",
    author = "Kwiatkowski, Tom  and
      Palomaki, Jennimaria  and
      Redfield, Olivia  and
      Collins, Michael  and
      Parikh, Ankur  and
      Alberti, Chris  and
      Epstein, Danielle  and
      Polosukhin, Illia  and
      Devlin, Jacob  and
      Lee, Kenton  and
      Toutanova, Kristina  and
      Jones, Llion  and
      Kelcey, Matthew  and
      Chang, Ming-Wei  and
      Dai, Andrew M.  and
      Uszkoreit, Jakob  and
      Le, Quoc  and
      Petrov, Slav",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1026",
    doi = "10.1162/tacl_a_00276",
    pages = "452--466",
    abstract = "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.",
}
** TODO Summary

* FactCHD
** Citation
** TODO Summary
* Retrieval Augmentation Reduces Hallucination in Conversation
** Citation
@misc{shuster2021retrieval,
      title={Retrieval Augmentation Reduces Hallucination in Conversation},
      author={Kurt Shuster and Spencer Poff and Moya Chen and Douwe Kiela and Jason Weston},
      year={2021},
      eprint={2104.07567},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
** TODO Summary

* Trusting Your Evidence: Hallucinate Less with Context-aware Decoding
** Citation
@misc{shi2023trusting,
      title={Trusting Your Evidence: Hallucinate Less with Context-aware Decoding},
      author={Weijia Shi and Xiaochuang Han and Mike Lewis and Yulia Tsvetkov and Luke Zettlemoyer and Scott Wen-tau Yih},
      year={2023},
      eprint={2305.14739},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
** TODO Summary

* How Language Model Hallucinations Can Snowball
** Citation
@misc{zhang2023language,
      title={How Language Model Hallucinations Can Snowball},
      author={Muru Zhang and Ofir Press and William Merrill and Alisa Liu and Noah A. Smith},
      year={2023},
      eprint={2305.13534},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
** TODO Summary

* FAITHDIAL: A Faithful Benchmark for Information-Seeking Dialogue
** Citation
@article{10.1162/tacl_a_00529,
    author = {Dziri, Nouha and Kamalloo, Ehsan and Milton, Sivan and Zaiane, Osmar and Yu, Mo and Ponti, Edoardo M. and Reddy, Siva},
    title = "{FaithDial: A Faithful Benchmark for Information-Seeking Dialogue}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {10},
    pages = {1473-1490},
    year = {2022},
    month = {12},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00529},
    url = {https://doi.org/10.1162/tacl\_a\_00529},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00529/2065956/tacl\_a\_00529.pdf},
}
** TODO Summary
* Improving Language Models with Advantage-based Offline Policy Gradients
** Citation
@misc{baheti2023improving,
      title={Improving Language Models with Advantage-based Offline Policy Gradients},
      author={Ashutosh Baheti and Ximing Lu and Faeze Brahman and Ronan Le Bras and Maarten Sap and Mark Riedl},
      year={2023},
      eprint={2305.14718},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
** TODO Summary
* Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models
** Citation
@misc{lin2023generating,
      title={Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models},
      author={Zhen Lin and Shubhendu Trivedi and Jimeng Sun},
      year={2023},
      eprint={2305.19187},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
** TODO Summary
* Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding
** Citation
@misc{dziri2021neural,
      title={Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding},
      author={Nouha Dziri and Andrea Madotto and Osmar Zaiane and Avishek Joey Bose},
      year={2021},
      eprint={2104.08455},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
** TODO Summary

* Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback
** Citation
@misc{tian2023just,
      title={Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback},
      author={Katherine Tian and Eric Mitchell and Allan Zhou and Archit Sharma and Rafael Rafailov and Huaxiu Yao and Chelsea Finn and Christopher D. Manning},
      year={2023},
      eprint={2305.14975},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
** TODO Summary
* Check Your Facts and Try Again: Improving LLM Models with External Knowledge and Automated Feedback
** Citation
@misc{peng2023check,
      title={Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback},
      author={Baolin Peng and Michel Galley and Pengcheng He and Hao Cheng and Yujia Xie and Yu Hu and Qiuyuan Huang and Lars Liden and Zhou Yu and Weizhu Chen and Jianfeng Gao},
      year={2023},
      eprint={2302.12813},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
** TODO Summary
